{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1sShCq1E3tEaMD5Dq1eW9mtdN9114vbt6",
      "authorship_tag": "ABX9TyP/ggUoE6Ty2FpQy7DVETY0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG4Hpn0QhzS0",
        "colab_type": "text"
      },
      "source": [
        "# Clean Data\n",
        "data format: txt,each line represent one news article  \n",
        "I write 2 clean function,  \n",
        "run one you like,I recommend first one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9brP-qjbYPD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\"\"\"\n",
        "  Clean text by regular expression\n",
        "  param text: the string of text\n",
        "  return: text string after cleaning without signel character\n",
        "  by Alex_NKG\n",
        "\"\"\"\n",
        "def clean_text(text):\n",
        "  text=re.sub(\"[^A-Za-z ]\",\"\",text)\n",
        "  flist=[]\n",
        "  t=text.split()\n",
        "  #print(t)\n",
        "  for i in t:\n",
        "    if len(i)>1:\n",
        "      flist.append(i)#remove single character\n",
        "  text = ' '.join(flist)\n",
        "  SnowballStemmer(\"english\").stem(text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry2uJSsVsFYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\"\"\"\n",
        "  Clean text by regular expression\n",
        "  param text: the string of text\n",
        "  return: text string after cleaning\n",
        "  fork from:https://blog.csdn.net/wmq104/article/details/82931352\n",
        "  simplify by Alex_NKG for news article clean\n",
        "\"\"\"\n",
        "def clean_text(text):\n",
        "    # acronym\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    text = re.sub(r\"can\\’t\", \"can not\", text)\n",
        "    text = re.sub(r\"cannot\", \"can not \", text)\n",
        "    text = re.sub(r\"what\\'s\", \"what is\", text)\n",
        "    text = re.sub(r\"What\\'s\", \"what is\", text)\n",
        "    text = re.sub(r\"\\'ve \", \" have \", text)\n",
        "    text = re.sub(r\"n\\'t\", \" not \", text)\n",
        "    text = re.sub(r\"i\\'m\", \"i am \", text)\n",
        "    text = re.sub(r\"I\\'m\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\",000\", '000', text)\n",
        "    text = re.sub(r\"\\'s|s\\'|’s|\\'|#|\\*\", \"\", text)\n",
        "\n",
        "    # spelling correction\n",
        "    text = re.sub(r\"donald trump\", \"trump\", text)\n",
        "    text = re.sub(r\"the big bang\", \"big-bang\", text)\n",
        "    text = re.sub(r\"the european union\", \"eu\", text)\n",
        "    text = re.sub(r\" usa \", \" america \", text)\n",
        "    text = re.sub(r\" us \", \" america \", text)\n",
        "    text = re.sub(r\" u s \", \" america \", text)\n",
        "    text = re.sub(r\" U\\.S\\. \", \" america \", text)\n",
        "    text = re.sub(r\" US \", \" america \", text)\n",
        "    text = re.sub(r\" American \", \" america \", text)\n",
        "    text = re.sub(r\" America \", \" america \", text)\n",
        "    text = re.sub(r\" 1 \", \" one \", text)\n",
        "    text = re.sub(r\" 2 \", \" two \", text)\n",
        "    text = re.sub(r\" 3 \", \" three \", text)\n",
        "    text = re.sub(r\" 4 \", \" four \", text)\n",
        "    text = re.sub(r\" 5 \", \" five \", text)\n",
        "    text = re.sub(r\" 6 \", \" six \", text)\n",
        "    text = re.sub(r\" 7 \", \" seven \", text)\n",
        "    text = re.sub(r\" 8 \", \" eight \", text)\n",
        "    text = re.sub(r\" 9 \", \" nine \", text)\n",
        "    text = re.sub(r\"the european union\", \" eu \", text)\n",
        "    text = re.sub(r\"dollars\", \" dollar \", text)\n",
        "    # symbol replacement\n",
        "    text = re.sub(r\"&\", \" and \", text)\n",
        "    text = re.sub(r\"\\|\", \" or \", text)\n",
        "    \n",
        "    text = re.sub(r\"\\$\", \" dollar \", text)\n",
        "    text = re.sub(r\"(\\.|,|\\?|;|!|\\–|\\-|\\(|\\))\", \" \", text)\n",
        "    text = re.sub(r\"(\\”)\", \" \", text)\n",
        "\n",
        "    # remove extra space\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUzlMpe1vaFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def clean_text_doc(fpath):\n",
        "  \"\"\"\n",
        "  clean text from txt\n",
        "  param text path\n",
        "  remove regulation based on function:clean_text\n",
        "  output np object\n",
        "  \"\"\"\n",
        "  f = open(fpath)\n",
        "  line = f.readline()\n",
        "  data_list = []\n",
        "  while line:\n",
        "    line=clean_text(line)\n",
        "    data = list(line.lower().split())\n",
        "    \n",
        "\n",
        "    data_list.append(data)\n",
        "    line = f.readline()\n",
        "  f.close()\n",
        "  #return data_list\n",
        "  #return np.array(data_list)\n",
        "  t=map(lambda x:\" \".join(x),np.array(data_list))\n",
        "  \n",
        "  return list(t)\n",
        "txt_data=clean_text_doc(\"/content/drive/My Drive/NLP/W7/article_dataset.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abbHs-dekiWE",
        "colab_type": "text"
      },
      "source": [
        "# Count Vectorization  \n",
        "Create a class that implements a count vectorizer. Your class must have the following methods:\n",
        "\n",
        "● The __init__ method, which initializes internal variables. Also takes as input:\n",
        "\n",
        "a. An n-gram option. You must take n=1,2, or 3. Default is 1. Test for values outside this range.\n",
        "\n",
        "b. Remove stop words. Default should be False.\n",
        "\n",
        "● A fit method, which will create a dictionary of terms which map to columns of the term-frequency matrix. Must take corpus as input.\n",
        "\n",
        "● A transform method, which will create a term-frequency matrix of appropriate size (num_docs times the vocab_size). This method takes no input, and returns a matrix.\n",
        "\n",
        "● A get_vocab size, which returns the dictionary of terms.\n",
        " \n",
        "I also test the running speed of the self CountVectorizer class, it run faster than orginal one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NP9Ks349UKH",
        "colab_type": "code",
        "outputId": "ecc8687e-f8af-4a68-8ea4-c7f233a81947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english')) \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#take in the stop word from NLTK"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLTBjimWkfVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "class MyCountVectorizer():\n",
        "  def __init__(self, pass_stop=False,n=1):\n",
        "      self.pass_stop = pass_stop\n",
        "      if n > 3:\n",
        "        raise Exception('n limite to 1 to 3,now n is {}'.format(n))# catch error\n",
        "      else:\n",
        "        self.n=n\n",
        "    \n",
        "    \n",
        "  def fit(self, data):#bulit dic : key:word of n grame,value: the index of word\n",
        "      self.use=data\n",
        "      data = map(lambda x:x.split(\" \"), data)\n",
        "      \n",
        "      self.elements_ = set()\n",
        "      clean_sw=[]\n",
        "      if self.pass_stop:\n",
        "        for i in data:\n",
        "          tmp=[]\n",
        "          for n in i:\n",
        "            if n not in stop_words:\n",
        "              tmp.append(n)\n",
        "          clean_sw.append(tmp)\n",
        "      \n",
        "      else: \n",
        "        clean_sw=data\n",
        "      \n",
        "      for i in clean_sw:\n",
        "        for n in range(1,self.n+1):\n",
        "          for j in range(0,len(i)-n+1):\n",
        "            x=\" \".join(i[j:j+n])\n",
        "            #print(x)\n",
        "            self.elements_.add(x)\n",
        "      #print(self.elements_)\n",
        "\n",
        "      self.elements_ = np.sort(list(self.elements_))\n",
        "      #print(self.elements_)\n",
        "      self.labels_ = np.arange(len(self.elements_)).astype(int)\n",
        "      #print(self.labels_)\n",
        "      self.dict_ = {}\n",
        "      for i in range(len(self.elements_)):\n",
        "        self.dict_[str(self.elements_[i])] = self.labels_[i]\n",
        "      #return self.dict_\n",
        "    \n",
        "  def transform(self):\n",
        "      \n",
        "      rows=[]\n",
        "      each_row_num=[0]\n",
        "      clean_sw=[]\n",
        "      vals=[]\n",
        "\n",
        "      data = (map(lambda x:x.split(\" \"), self.use))\n",
        "\n",
        "      if self.pass_stop:\n",
        "        for i in data:\n",
        "          tmp=[]\n",
        "          for n in i:\n",
        "            if n not in stop_words:\n",
        "              tmp.append(n)\n",
        "          clean_sw.append(tmp)\n",
        "      \n",
        "      else: \n",
        "        clean_sw=data\n",
        "      #print(clean_sw)\n",
        "      for i in clean_sw:\n",
        "        single_rows=[]\n",
        "        single_cols = []\n",
        "        for n in range(1,self.n+1):\n",
        "          count=0\n",
        "          for j in range(0,len(i)-n+1):\n",
        "            x=\" \".join(i[j:j+n])\n",
        "            rows.append(self.dict_[x])\n",
        "            count=count+1 \n",
        "          tmp=each_row_num.pop()\n",
        "          each_row_num.append(tmp)\n",
        "          each_row_num.append(tmp+count)\n",
        "          \n",
        "          \n",
        "      vals = np.ones((len(rows),)).astype(int)\n",
        "      rows=np.array(rows).astype(int)   \n",
        "      each_row_num=np.array(each_row_num).astype(int) \n",
        "\n",
        "      return sparse.csr_matrix((vals, rows, each_row_num), dtype=int)\n",
        "  def get_vocab(self):\n",
        "    return self.dict_\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDHF-PUEdO1R",
        "colab_type": "text"
      },
      "source": [
        "上述方式为按照row行来压缩\n",
        "（1）data表示数据，为[1, 2, 3, 4, 5, 6]  \n",
        "（2）shape表示矩阵的形状  \n",
        "（3）indices表示对应data中的数据，在压缩后矩阵中各行的下标，如：数据1在某行的0位置处，数据2在某行的2位置处，数据6在某行的2位置处。  \n",
        "（4）indptr表示压缩后矩阵中每一行所拥有数据的个数，如：[0 2 3 6]表示从第0行开始数据的个数，0表示默认起始点，0之后有几个数字就表示有几行，第一个数字2表示第一行有2 - 0 = 2个数字，因而数字1，2都第0行，第二行有3 - 2 = 1个数字，因而数字3在第1行，以此类推。  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bG0H1jKHX42",
        "colab_type": "code",
        "outputId": "ef250984-9eb5-4a75-bc58-83ece126e9c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#test with small fine formated data set\n",
        "xx=txt_data[0:50]\n",
        "mcv = MyCountVectorizer()\n",
        "mcv.fit(xx)\n",
        "k1=mcv.transform()\n",
        "cv=CountVectorizer()\n",
        "cv.fit(xx)\n",
        "k2=cv.transform(xx)\n",
        "np.sum(k1-k2) == 0#check correct"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52CM45d-ffsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mcv = MyCountVectorizer(pass_stop=True)\n",
        "mcv.fit(xx)\n",
        "v1=mcv.transform()\n",
        "w=mcv.get_vocab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "86104e1a-0102-44f2-b066-1ffe6dc2d4ab",
        "id": "rfSnAgoKPoB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import time\n",
        "import numpy as np\n",
        "# CountVectorizer in sklearn\n",
        "start1 = time.time()\n",
        "cv = CountVectorizer()\n",
        "cv.fit(txt_data)\n",
        "v1=cv.transform(txt_data)\n",
        "end1 = time.time()\n",
        "# MyCountVectorizer\n",
        "start2 = time.time()\n",
        "mcv = MyCountVectorizer()\n",
        "mcv.fit(txt_data)\n",
        "v2=mcv.transform()\n",
        "end2 = time.time()\n",
        "q1=end1-start1\n",
        "q2=end2-start2\n",
        "print('check result if the same as orginal function:',np.sum(v2-v1) == 0)\n",
        "print(\"Orginal function time :\",q1,\"\\n\",\"My function time:\",q2)\n",
        "#My function runs faster"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check result if the same as orginal function: True\n",
            "Orginal function time : 5.1643226146698 \n",
            " My function time: 4.114795923233032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76m9K5dpN4DV",
        "colab_type": "text"
      },
      "source": [
        "# Latent Semantic Analysis  \n",
        "Create a class that implements LSA. You must implement using numpy’s SVD functions. Your class must have the following methods:\n",
        "\n",
        "● The __init__ method, which initializes internal variables. Also takes as input:\n",
        "\n",
        "a. The term-frequency matrix, A\n",
        "\n",
        "b. The desired number of topics (or rank), k\n",
        "\n",
        "● A transform method, which yields the topic distribution per document\n",
        "\n",
        "● A get_topics method, which will return the top n words for all k topics as a list of lists\n",
        "\n",
        "● A get_importance method, which will return the importance of all k topics as a list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GAu3jdERFbd",
        "colab_type": "text"
      },
      "source": [
        "![替代文字](https://i.loli.net/2020/03/11/cCAsKeUW8rQ59NO.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4PzlaUikrbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSA():\n",
        "  def __init__(self, A,k):\n",
        "    self.A=A\n",
        "    self.k=k\n",
        "\n",
        "  def transform(self):\n",
        "    u, s, vt = np.linalg.svd(self.A.toarray().T, full_matrices=False)\n",
        "    new_U  = u[:len(u), :self.k]\n",
        "    new_VT = vt[:self.k, :len(vt)]\n",
        "    new_S=np.diag(s[0:self.k])\n",
        "    self.mat=np.dot(new_U, np.dot(new_S, new_VT)).T\n",
        "    return self.mat\n",
        "  def get_topics(self,n,dic):#I believe we need input the dict of the word here use the previous: get_vocab()\n",
        "    top_n=[]\n",
        "    rev_dic=dict(zip(dic.values(), dic.keys()))\n",
        "    for i in self.A.toarray():\n",
        "      #print(i)\n",
        "      tmp=[]\n",
        "      argesort=np.argsort(i)\n",
        "      e= argesort[::-1]\n",
        "      #print(e)\n",
        "      for ks in range(0,n):\n",
        "        tmp.append(rev_dic[e[ks]])\n",
        "      top_n.append(tmp)\n",
        "    return top_n    \n",
        "\n",
        "  def get_importance():\n",
        "    pass\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhmgjlZytR53",
        "colab_type": "code",
        "outputId": "68075c0c-a964-47f3-8f91-72a72eec81c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "l=LSA(v1,k=2)\n",
        "l.transform()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.20395146e-03, 6.01975729e-04, 3.00631204e-03, ...,\n",
              "        3.01362524e-02, 1.96943475e-03, 2.22095929e-03],\n",
              "       [6.60403314e-04, 3.30201657e-04, 1.41727887e-03, ...,\n",
              "        1.91448180e-02, 9.85236822e-04, 1.10641979e-03],\n",
              "       [5.61564531e-04, 2.80782266e-04, 1.38304590e-03, ...,\n",
              "        1.42731676e-02, 9.10737132e-04, 1.02666621e-03],\n",
              "       ...,\n",
              "       [1.11281983e-04, 5.56409914e-05, 2.62787797e-04, ...,\n",
              "        2.95568549e-03, 1.75848255e-04, 1.98004139e-04],\n",
              "       [5.18040953e-04, 2.59020477e-04, 1.06630040e-03, ...,\n",
              "        1.55305087e-02, 7.54207096e-04, 8.45974626e-04],\n",
              "       [1.28828995e-03, 6.44144977e-04, 3.06832618e-03, ...,\n",
              "        3.39232088e-02, 2.04645848e-03, 2.30484187e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKQ09YiRzQgR",
        "colab_type": "code",
        "outputId": "cbac92b3-fd70-43f6-b2f7-69a9fbb879cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "l.get_topics(n=6,dic=w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['barclays', 'us', 'bn', 'bank', 'staley', 'asked'],\n",
              " ['poll', 'lead', 'trump', 'clintons', 'days', 'prriatlantic'],\n",
              " ['zika', 'yet', 'latin', 'ebola', 'church', 'jail'],\n",
              " ['cruz', 'rubio', 'win', 'state', 'trump', 'senator'],\n",
              " ['election', 'us', 'trump', 'clinton', 'republican', 'day'],\n",
              " ['twitter', 'account', 'spencer', 'accounts', 'reinstated', 'multiple'],\n",
              " ['grande', 'ariana', 'donuts', 'licking', 'star', 'saying'],\n",
              " ['car', 'ad', 'song', 'advert', 'music', 'red'],\n",
              " ['said', 'africa', 'health', 'grants', 'innovation', 'waiswa'],\n",
              " ['tax', 'spanish', 'accounts', 'information', 'bank', 'santander'],\n",
              " ['clinton', 'trump', 'new', 'sanders', 'women', 'said'],\n",
              " ['city', 'west', 'agero', 'second', 'ham', 'first'],\n",
              " ['clubs', 'prices', 'away', 'ticket', 'league', 'premier'],\n",
              " ['deutsche', 'uk', 'bn', 'banks', 'share', 'business'],\n",
              " ['email', 'first', 'mail', 'released', 'users', 'internet'],\n",
              " ['banks', 'italian', 'could', 'dei', 'monte', 'sector'],\n",
              " ['customers', 'banks', 'banking', 'said', 'cma', 'competition'],\n",
              " ['us', 'trump', 'organic', 'month', 'republican', 'donald'],\n",
              " ['park', 'reading', 'tour', 'leeds', 'aug', 'first'],\n",
              " ['vote', 'remain', 'polling', 'leave', 'said', 'referendum'],\n",
              " ['band', 'though', 'playing', 'radio', 'kicks', 'undertones'],\n",
              " ['eu', 'government', 'market', 'brexit', 'deal', 'single'],\n",
              " ['children', 'one', 'grandchildren', 'didnt', 'people', 'would'],\n",
              " ['labour', 'eu', 'workers', 'wages', 'pay', 'migration'],\n",
              " ['sunderland', 'game', 'scott', 'defoe', 'swansea', 'point'],\n",
              " ['eastern', 'afar', 'streets', 'first', 'compelling', 'boys'],\n",
              " ['eu', 'said', 'would', 'uk', 'prime', 'britain'],\n",
              " ['trump', 'israel', 'donors', 'proisrael', 'adelson', 'jewish'],\n",
              " ['turnbull', 'australia', 'said', 'government', 'campaign', 'coalition'],\n",
              " ['never', 'gallagher', 'back', 'record', 'said', 'made'],\n",
              " ['brexit', 'minister', 'would', 'made', 'leave', 'article'],\n",
              " ['one', 'like', 'film', 'theres', 'tells', 'toni'],\n",
              " ['ae', 'performance', 'patients', 'hospitals', 'care', 'health'],\n",
              " ['trump', 'ad', 'republican', 'sanders', 'said', 'justice'],\n",
              " ['son', 'stoke', 'hughes', 'goal', 'tottenham', 'first'],\n",
              " ['would', 'economy', 'brexit', 'vote', 'remain', 'uk'],\n",
              " ['scotland', 'rocknroll', 'rock', 'labour', 'radio', 'one'],\n",
              " ['financial', 'carney', 'bank', 'banks', 'shadow', 'banking'],\n",
              " ['heard', 'act', 'like', 'depp', 'joyce', 'time'],\n",
              " ['new', 'season', 'league', 'club', 'last', 'players'],\n",
              " ['kiarostami', 'film', 'films', 'women', 'people', 'tehran'],\n",
              " ['films', 'silent', 'cinema', 'would', 'drama', 'projector'],\n",
              " ['mental', 'health', 'care', 'nhs', 'funding', 'transformation'],\n",
              " ['pathology', 'government', 'savings', 'said', 'report', 'companies'],\n",
              " ['air', 'holgate', 'says', 'may', 'formaldehyde', 'people'],\n",
              " ['theres', 'mine', 'time', 'malik', 'like', 'mind'],\n",
              " ['cash', 'would', 'rogoff', 'currency', 'payments', 'payment'],\n",
              " ['hits', 'fat', 'white', 'songs', 'mothers', 'modern'],\n",
              " ['film', 'riots', 'craig', 'berry', 'also', 'play'],\n",
              " ['tek', 'birdman', 'music', 'australia', 'radio', 'us']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ctg3bLMHsIN",
        "colab_type": "code",
        "outputId": "be9268ca-5c77-4ddb-c075-c837cdaa744d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "k1.toarray().shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdwE4Dy-DcsA",
        "colab_type": "code",
        "outputId": "8187aa84-a011-49a6-e3b4-c65a3e2a2239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3.01511345e-01  2.24808511e-17 -5.07552879e-17]\n",
            " [-1.11022302e-16 -2.54480786e-01 -2.98472249e-01]\n",
            " [-1.17055201e-16 -1.83419373e-01  4.14108124e-01]\n",
            " [-5.55111512e-17 -6.92380945e-01 -1.82836373e-01]\n",
            " [ 2.86615711e-17 -1.83419373e-01  4.14108124e-01]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [-1.38777878e-17 -4.37900159e-01  1.15635876e-01]\n",
            " [-4.16333634e-17 -2.54480786e-01 -2.98472249e-01]\n",
            " [-4.16333634e-17 -2.54480786e-01 -2.98472249e-01]\n",
            " [ 2.86615711e-17 -1.83419373e-01  4.14108124e-01]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [ 2.77555756e-17 -1.83419373e-01  4.14108124e-01]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]\n",
            " [ 3.01511345e-01  0.00000000e+00  0.00000000e+00]]\n",
            "(20, 3)\n",
            "-------\n",
            "[[ 0.          0.          1.        ]\n",
            " [-0.81124219 -0.58471028 -0.        ]\n",
            " [-0.58471028  0.81124219  0.        ]]\n",
            "(3, 3)\n",
            "-------\n",
            "[3.31662479 3.18783275 1.95901055]\n",
            "(3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqBWao-Meq9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}