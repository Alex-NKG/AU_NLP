{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lab8 + HW6.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZDWsXf_LRAB",
        "colab_type": "text"
      },
      "source": [
        "<H2> Intro to Applied NLP -- Lab 8 + HW 6 </H2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kjZzof7LRAE",
        "colab_type": "text"
      },
      "source": [
        "<H3> Similarity Measures, Revisited </H3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLgyA0YeLRAF",
        "colab_type": "text"
      },
      "source": [
        "<H4> Name: YUEYANG LIU</H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMF_Cf9kLRAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the 20 newsgroups dataset is common these days -- let's start here\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train') #takes a bit...\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXZESygiLRAJ",
        "colab_type": "code",
        "outputId": "91b48da0-77ac-4c17-919e-53d435c2e618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# let's see how the data is categorized\n",
        "newsgroups_test.target_names==newsgroups_train.target_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTVyEVaMLRAN",
        "colab_type": "code",
        "outputId": "4c67d54d-e93c-43ca-c0b0-a59f3654d598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "# let's look at one entry of the data\n",
        "print(newsgroups_train.data[9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\n",
            "Subject: Re: Sigma Designs Double up??\n",
            "Article-I.D.: ux1.C52u8x.B62\n",
            "Organization: University of Illinois at Urbana\n",
            "Lines: 29\n",
            "\n",
            "jap10@po.CWRU.Edu (Joseph A. Pellettiere) writes:\n",
            "\n",
            "\n",
            ">\tI am looking for any information about the Sigma Designs\n",
            ">\tdouble up board.  All I can figure out is that it is a\n",
            ">\thardware compression board that works with AutoDoubler, but\n",
            ">\tI am not sure about this.  Also how much would one cost?\n",
            "\n",
            "I've had the board for over a year, and it does work with Diskdoubler,\n",
            "but not with Autodoubler, due to a licensing problem with Stac Technologies,\n",
            "the owners of the board's compression technology. (I'm writing this\n",
            "from memory; I've lost the reference. Please correct me if I'm wrong.)\n",
            "\n",
            "Using the board, I've had problems with file icons being lost, but it's\n",
            "hard to say whether it's the board's fault or something else; however,\n",
            "if I decompress the troubled file and recompress it without the board,\n",
            "the icon usually reappears. Because of the above mentioned licensing\n",
            "problem, the freeware expansion utility DD Expand will not decompress\n",
            "a board-compressed file unless you have the board installed.\n",
            "\n",
            "Since Stac has its own product now, it seems unlikely that the holes\n",
            "in Autodoubler/Diskdoubler related to the board will be fixed.\n",
            "Which is sad, and makes me very reluctant to buy Stac's product since\n",
            "they're being so stinky. (But hey, that's competition.)\n",
            "-- \n",
            "\n",
            "Stan Kerr    \n",
            "Computing & Communications Services Office, U of Illinois/Urbana\n",
            "Phone: 217-333-5217  Email: stankerr@uiuc.edu   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw2gB0l6LRAQ",
        "colab_type": "code",
        "outputId": "41fc66d5-0b92-4b5a-db47-4ab3dbfe4df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# let's look at a few entries of the data categories\n",
        "newsgroups_train.target[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3carzKacQLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U8KbpztLRAT",
        "colab_type": "code",
        "outputId": "ff0b8fe6-3eb4-452e-e57e-e6ba1c18eb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# let's examine how many documents there are!\n",
        "len(newsgroups_train.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJwHWRa8LRAW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<H3> The usual pre-processing </H3>\n",
        "\n",
        "1. Create a function that cleans the text data. Be sure to remove things like email addresses/internet domains.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DIS13Ayrfj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    # url email\n",
        "    text=re.sub(r'(((https|http|ftp|rtsp|mms)?:\\/\\/)[^\\s]+)|(\\w[-\\w.+]*@([A-Za-z0-9][-A-Za-z0-9]+\\.)+[A-Za-z]{2,14})', \"\", text)\n",
        "\n",
        "    # acronym\n",
        "    #text = re.sub(r\"\\n\", \"\", text)\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    text = re.sub(r\"can\\’t\", \"can not\", text)\n",
        "    text = re.sub(r\"cannot\", \"can not \", text)\n",
        "    text = re.sub(r\"what\\'s\", \"what is\", text)\n",
        "    text = re.sub(r\"What\\'s\", \"what is\", text)\n",
        "    text = re.sub(r\"\\'ve \", \" have \", text)\n",
        "    text = re.sub(r\"n\\'t\", \" not \", text)\n",
        "    text = re.sub(r\"i\\'m\", \"i am \", text)\n",
        "    text = re.sub(r\"I\\'m\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\",000\", '000', text)\n",
        "    text = re.sub(r\"\\'s|s\\'|’s|\\'|#|\\*|:|>\", \"\", text)\n",
        "    \n",
        "    # symbol replacement\n",
        "    text = re.sub(r\"&\", \" and \", text)\n",
        "    text = re.sub(r\"\\|\", \" or \", text)\n",
        "    text = re.sub(r\"\\$\", \" dollar \", text)\n",
        "    text = re.sub(r\"(\\.|,|\\?|;|!|\\–|\\-|\\(|\\))\", \" \", text)\n",
        "    #text = re.sub(r\"(\\”)\", \" \", text)\n",
        "    text=text.lower()\n",
        "    # remove extra space\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rultBo2nd3YZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data,tag = fetch_20newsgroups(shuffle=True, \n",
        "                              #random_state=1,\n",
        "                             remove=('headers', 'footers', 'quotes'),\n",
        "                             return_X_y=True,\n",
        "                              subset='train')\n",
        "clean_data=[]\n",
        "for i in data[:1000]: clean_data.append(clean_text(i))\n",
        "data_samples = clean_data\n",
        "data_samples=data[:1000]\n",
        "#sampletag=tag[:1000]\n",
        "sampletag=tag[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go8XZqt0lDQY",
        "colab_type": "code",
        "outputId": "5471e28a-cda9-4fa3-db7b-c072ca3dab49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tag[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFan4ydMceoz",
        "colab_type": "code",
        "outputId": "5cc5e259-ed99-4c21-80e9-3e4929be4ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sampletag[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WocU_2mQLRAX",
        "colab_type": "text"
      },
      "source": [
        "<H3> Topic Modeling </H3>\n",
        "\n",
        "2. Use sklearn's CountVectorizer and TFIDFVectorizer to transform the text data to a matrix:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmdxQT4fs9rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "n_samples = len(data)\n",
        "n_features = 1000\n",
        "n_components = 10\n",
        "n_top_words = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab5RSV1An7Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "                                  max_df=0.85, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uCC5lrOoUKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_vectorizer = CountVectorizer(\n",
        "                                max_df=0.85, min_df=2,\n",
        "                                #max_features=n_features,\n",
        "                                stop_words='english')\n",
        "\n",
        "tf = tf_vectorizer.fit_transform(data_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxi0c66_suXQ",
        "colab_type": "text"
      },
      "source": [
        "3. Follow along this code, to apply NMF to the corpus:\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHWAoJx5s-aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF, LatentDirichletAllocation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsFxFmJPaomw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq8_-tk5axkQ",
        "colab_type": "code",
        "outputId": "b6fee4a4-b47e-4e45-85f5-cbf7849110c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "nmf = NMF(n_components=n_components, #must add commpoenents num or high ram use\n",
        "          random_state=1,\n",
        "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "nmf_ma=nmf.fit_transform(tfidf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topics in NMF model (Frobenius norm):\n",
            "Topic #0: don know like people good time ve really use way want does things say right problem new ll used make\n",
            "Topic #1: thanks mail info information advance address appreciated anybody looking greatly hi help need appreciate subject knows know edu does mac\n",
            "Topic #2: god jesus truth faith christian says believe john sin hear nature father man people christ christianity did follow christians bible\n",
            "Topic #3: armenian armenians turkish genocide soviet government armenia people turks russian muslim population million women argic serdar children war kurds 000\n",
            "Topic #4: windows files program file ftp use run pc image dos software microsoft code called disk nt write using data win\n",
            "Topic #5: just tell oh version work did guess new water used reading speed bit wanted simms little try turkish kind price\n",
            "Topic #6: sale offer 00 condition asking cable disks best price interested 15 make new email excellent edu included cd modem respond\n",
            "Topic #7: game team runs games hockey year win season night play players doesn bad league better morris great mark end years\n",
            "Topic #8: com long time mail contact access request lower people article systems quite organization try reply warning doesn brian address edu\n",
            "Topic #9: think don better agree sex way state reason expected year red nice day easily lost won controller try bad argument\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OK0v9rbsuMc",
        "colab_type": "text"
      },
      "source": [
        "4. Compare NMF Results with LSA Results, when using TFIDF Vectorizer, with n-ranges: (1,1), (1,2), (2,3). Which yield the best (empirical) results? You may have to play around with the TFIDF Vectorizer's min/max df parameter. (Don't forget about the stopword removal!)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omdhk9f8hHbw",
        "colab_type": "code",
        "outputId": "be5494ca-a501-4316-935d-78e303419316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "terms = tfidf_vectorizer.get_feature_names()\n",
        "lsa = TruncatedSVD(n_components)               \n",
        "X2 = lsa.fit_transform(tfidf)\n",
        "n_pick_docs= 1\n",
        "topic_docs_id = [X2[:,t].argsort()[:-(n_pick_docs+1):-1] for t in range(n_components)]\n",
        "topic_keywords_id = [lsa.components_[t].argsort()[:-(n_top_words+1):-1] for t in range(n_components)]\n",
        "print(\"Topics in LSA\")\n",
        "for t in range(n_components):\n",
        "  print(\"topic #%d:\" % t,\" \".join(terms[topic_keywords_id[t][j]] for j in range(n_top_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topics in LSA\n",
            "topic #0: don just know think like people good time use ve does thanks really new way did god want problem say\n",
            "topic #1: thanks windows mail files program use file card information edu info software advance pc looking address com problem need anybody\n",
            "topic #2: god thanks jesus mail truth com people christian says does know windows faith believe information address program advance greatly christianity\n",
            "topic #3: armenian armenians turkish government people com genocide thanks soviet edu information mail address armenia new 000 turks 00 children russian\n",
            "topic #4: thanks just mail good info looking sale game address god offer season information team oh tell think bike price year\n",
            "topic #5: just problem card turkish armenian armenians memory thanks god video simms used did jesus mac soviet genocide info ram new\n",
            "topic #6: jesus god sale new 00 game make offer year good 10 games condition power 20 15 12 team john asking\n",
            "topic #7: think com just edu year data new read problem program try window jesus systems won long organization nasa send code\n",
            "topic #8: think don use help year thanks advance armenian info windows better appreciated armenians agree file 00 pc turkish files need\n",
            "topic #9: com sale drive offer cable just window don right want time box work disks mail contact make try modem people\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK1vBSJjaqh0",
        "colab_type": "code",
        "outputId": "3ba529d6-f456-4536-8946-b283cd18c284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
        "                                  max_df=0.95, min_df=2,\n",
        "                                   #max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "terms = tfidf_vectorizer.get_feature_names()\n",
        "lsa = TruncatedSVD(n_components)               \n",
        "X2 = lsa.fit_transform(tfidf)\n",
        "n_pick_docs= 1\n",
        "topic_docs_id = [X2[:,t].argsort()[:-(n_pick_docs+1):-1] for t in range(n_components)]\n",
        "topic_keywords_id = [lsa.components_[t].argsort()[:-(n_top_words+1):-1] for t in range(n_components)]\n",
        "print(\"Topics in LSA\")\n",
        "for t in range(n_components):\n",
        "  print(\"topic #%d:\" % t,\" \".join(terms[topic_keywords_id[t][j]] for j in range(n_top_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topics in LSA\n",
            "topic #0: don know just people think like good use time does ve god thanks did really way new problem want jesus\n",
            "topic #1: god jesus people truth think believe armenian christian armenians say don said faith argument turkish bible did man religion true\n",
            "topic #2: cadre dsl intellect geb shameful surrender shameful edu shameful geb cadre cadre dsl pitt dsl surrender soon geb n3jxp skepticism chastity intellect chastity banks n3jxp skepticism chastity n3jxp gordon banks pitt pitt edu\n",
            "topic #3: armenian armenians turkish genocide soviet government armenia turks 000 russian people soviet armenian armenian government population muslim war killed million turkey women\n",
            "topic #4: god jesus armenian windows armenians turkish thanks program mail file files genocide christian people soviet truth info faith image address\n",
            "topic #5: thanks mail 00 sale offer god jesus address mail thanks looking game good best offer info year com information condition price interested\n",
            "topic #6: jesus card problem god video machine memory mac simms drive ram disk 00 software sale game color offer new scsi\n",
            "topic #7: windows 00 file game program team files year problems games win printer good run runs hockey image microsoft season players\n",
            "topic #8: use data jesus clipper chip key space sale keys nasa government clipper chip shuttle offer new house com computer phone 00\n",
            "topic #9: don windows 00 sale good offer think car disks looking don know best offer make offer know don think price asking drive rights people\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWXqJhBWfbo0",
        "colab_type": "code",
        "outputId": "4307cb00-9eaa-43d6-840f-1521ab24ae96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "nmf = NMF(n_components=n_components, #must add commpoenents num or high ram use\n",
        "          random_state=1,\n",
        "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "nmf_ma=nmf.fit_transform(tfidf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topics in NMF model (Frobenius norm):\n",
            "Topic #0: don know think just people like good ve god time really does way want say did things use believe right\n",
            "Topic #1: simms use pin machine memory 80ns 80ns simms crashes bk 50mhz bank apple 30 mac 70ns lc meg slower faster need\n",
            "Topic #2: dsl pitt cadre dsl dsl shameful surrender geb cadre edu shameful intellect geb surrender soon cadre shameful geb skepticism chastity banks n3jxp n3jxp skepticism n3jxp chastity intellect chastity gordon banks pitt pitt edu\n",
            "Topic #3: armenian armenians turkish genocide soviet armenia turks people russian soviet armenian government armenian government muslim turkey soviet armenia population million 000 kurds women\n",
            "Topic #4: jesus god john matthew sin faith king father kingdom apostles jesus did says death punishment people messiah did david passage man\n",
            "Topic #5: thanks mail mail thanks address howard looking thanks advance com info advance reader font needed thanks knows let know line information know mail address anybody\n",
            "Topic #6: card video video card color driver monitor mac vga chipset colors display cica suffice accelerated software slot hi bit looking used\n",
            "Topic #7: windows files program file ini nt microsoft ini files windows nt updating files windows utility directory run os ftp windows directory code dos source code\n",
            "Topic #8: clipper semiconductor fairchild fairchild semiconductor chip clipper chip intergraph isn key clinton tim civil supplier risc liberties acquired announcement lawyer referring workstations\n",
            "Topic #9: 00 15 00 15 shipping 20 00 10 00 dr amp wr games ms 20 10 250 include sale version software du prices include\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt1s4kVBjTn-",
        "colab_type": "code",
        "outputId": "8fa5cc14-c7a7-460b-9f20-c0007f999bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
        "                                  max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "terms = tfidf_vectorizer.get_feature_names()\n",
        "lsa = TruncatedSVD(n_components)               \n",
        "X2 = lsa.fit_transform(tfidf)\n",
        "n_pick_docs= 1\n",
        "topic_docs_id = [X2[:,t].argsort()[:-(n_pick_docs+1):-1] for t in range(n_components)]\n",
        "topic_keywords_id = [lsa.components_[t].argsort()[:-(n_top_words+1):-1] for t in range(n_components)]\n",
        "print(\"Topics in LSA\")\n",
        "for t in range(n_components):\n",
        "  print(\"topic #%d:\" % t,\" \".join(terms[topic_keywords_id[t][j]] for j in range(n_top_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topics in LSA\n",
            "topic #0: don just know think like people good time use ve does thanks really new way did god want problem say\n",
            "topic #1: thanks windows mail files program use file card edu information info advance software pc looking address com problem anybody using\n",
            "topic #2: god jesus thanks mail truth com christian windows says does people faith believe information know advance address greatly question christianity\n",
            "topic #3: armenian armenians turkish government thanks people com genocide soviet edu information mail address 00 000 armenia new russian turks children\n",
            "topic #4: thanks just mail good looking game god season sale info team oh address tell offer hockey information great 00 year\n",
            "topic #5: just problem simms card god new turkish did memory work used tell jesus armenian thanks armenians mac info video water\n",
            "topic #6: don know thanks just com don know mail window want tell right think turkish armenian people government way armenians let info\n",
            "topic #7: think com problem don new don think edu way send state 00 year sale price time agree program sex jesus argument\n",
            "topic #8: post like team game files did got runs edu games ve windows file hockey season program doesn problems university just\n",
            "topic #9: com just new edu chip clipper key phone data time article bit isn doesn using long really access keys need\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5W3mfFMfdda",
        "colab_type": "code",
        "outputId": "3c4239a9-90d2-4e46-a47a-a0c5f6d9c510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "nmf = NMF(n_components=n_components, #must add commpoenents num or high ram use\n",
        "          random_state=1,\n",
        "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "nmf_ma=nmf.fit_transform(tfidf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topics in NMF model (Frobenius norm):\n",
            "Topic #0: don know like people good time ve use really want way does things say right problem new ll did make\n",
            "Topic #1: thanks mail info information advance address appreciated anybody looking greatly hi help need appreciate subject edu knows know does mac\n",
            "Topic #2: god jesus truth faith christian says believe sin john hear nature man people father christ christianity did christians follow bible\n",
            "Topic #3: armenian armenians turkish genocide soviet government armenia people turks soviet armenian russian armenian government muslim population million argic women serdar serdar argic 000\n",
            "Topic #4: windows files program file ftp use run pc image dos microsoft software code called disk write nt using data win\n",
            "Topic #5: just tell oh version work did guess new water used reading speed bit wanted simms little try turkish price kind\n",
            "Topic #6: sale offer 00 condition asking cable disks best price interested new 15 make email edu excellent included cd modem respond\n",
            "Topic #7: game team runs games hockey year win season night play doesn bad players league better morris great mark years end\n",
            "Topic #8: think don think don better agree sex way state reason expected year argument red easily lost day wrong bad try won\n",
            "Topic #9: com long time mail contact access request lower people article systems quite organization try reply warning doesn brian address draw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSGCWrOSiQNo",
        "colab_type": "text"
      },
      "source": [
        "according to the result the (1:2)ngream waorks well if we raise to (1:3) we can't see a improvement and during the raise of ngram the NMF don't see a improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPc3JUVlLRAY",
        "colab_type": "text"
      },
      "source": [
        "<H3> Information Retrieval </H3>  \n",
        "5. Use a clustering algorithm to group documents:  \n",
        "\n",
        "a) by the Count Vectorized Form  \n",
        "b) by the TF-IDF Vectorized output  \n",
        "c) by the similarity of their NMF topic distribution, for the ngram ranges identified in step 6.   \n",
        "\n",
        "Compare results from Agglomerative Clustering and k-means clustering. Also experiment with the number of clusters you expect the algorithm to find. \n",
        "\n",
        "Hint 1: the number of clusters should be similar to the amount of categories in this dataset.\n",
        "\n",
        "Hint 2: There are at least 6 experiments you have to compare here. Be sure to keep track of them all appropriately.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbGeqFoK0-_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics  \n",
        "def calculate_result(actual,pred):  \n",
        "  m_precision = metrics.precision_score(actual,pred, average='weighted')\n",
        "  m_recall = metrics.recall_score(actual,pred,average='weighted')\n",
        "  print ('predict info:'  )\n",
        "  print ('precision:{0:.3f}'.format(m_precision)  )\n",
        "  print ('recall:{0:0.3f}'.format(m_recall))\n",
        "  print ('f1-score:{0:.3f}'.format(metrics.f1_score(actual,pred,average='weighted')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMjq6mT6t5FB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def km_j(labels,km_labels_,tfidf):\n",
        "  print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km_labels_))\n",
        "  print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km_labels_))\n",
        "  print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km_labels_))\n",
        "  print(\"Silhouette Coefficient: %0.3f\"% metrics.silhouette_score(tfidf, km_labels_, sample_size=1000))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYfXoQHN_SyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "cv_ac = AgglomerativeClustering(n_clusters=20).fit(tf.toarray())\n",
        "cv_ac_p=cv_ac.fit_predict(tf.toarray())\n",
        "tf_ac = AgglomerativeClustering(n_clusters=20).fit(tfidf.toarray())\n",
        "tf_ac_p=tf_ac.fit_predict(tfidf.toarray())\n",
        "nmf_ac = AgglomerativeClustering(n_clusters=20).fit(nmf_ma)\n",
        "nmf_ac_p=nmf_ac.fit_predict(nmf_ma)\n",
        "lsa_ac= AgglomerativeClustering(n_clusters=20).fit(X2)\n",
        "lsa_ac_p=lsa_ac.fit_predict(X2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luNl4-NiUQlF",
        "colab_type": "code",
        "outputId": "b95089f2-9ec6-410a-b977-cdcf4dbf6895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "km_j(sampletag,cv_ac_p,tf)\n",
        "calculate_result(sampletag,cv_ac_p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.039\n",
            "Completeness: 0.346\n",
            "V-measure: 0.071\n",
            "Silhouette Coefficient: 0.554\n",
            "predict info:\n",
            "precision:0.003\n",
            "recall:0.045\n",
            "f1-score:0.005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2CToJvXUQZ0",
        "colab_type": "code",
        "outputId": "92e4edb2-609b-4f27-a48b-d2b00682eb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "km_j(sampletag,tf_ac_p,tfidf)\n",
        "calculate_result(sampletag,tf_ac_p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.208\n",
            "Completeness: 0.302\n",
            "V-measure: 0.246\n",
            "Silhouette Coefficient: 0.015\n",
            "predict info:\n",
            "precision:0.057\n",
            "recall:0.046\n",
            "f1-score:0.034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_SvMKPSUQRV",
        "colab_type": "code",
        "outputId": "429b4f7f-0867-4a75-f957-42d35d759489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "km_j(sampletag,nmf_ac_p,nmf_ma)\n",
        "calculate_result(sampletag,nmf_ac_p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.188\n",
            "Completeness: 0.242\n",
            "V-measure: 0.212\n",
            "Silhouette Coefficient: 0.269\n",
            "predict info:\n",
            "precision:0.046\n",
            "recall:0.063\n",
            "f1-score:0.046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwfOHv4sUeax",
        "colab_type": "code",
        "outputId": "12dfa728-1f66-4efe-ae05-9e65445215db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "km_j(sampletag,lsa_ac_p,X2)\n",
        "calculate_result(sampletag,lsa_ac_p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.258\n",
            "Completeness: 0.276\n",
            "V-measure: 0.267\n",
            "Silhouette Coefficient: 0.113\n",
            "predict info:\n",
            "precision:0.093\n",
            "recall:0.064\n",
            "f1-score:0.070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrza8zHtUm9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "cv_me = KMeans(n_clusters=20, random_state=0).fit(tf)\n",
        "cv_me_p=cv_me.predict(tf)\n",
        "tf_me = KMeans(n_clusters=20, random_state=0).fit(tfidf)\n",
        "tf_me_p=tf_me.predict(tfidf)\n",
        "nmf_me = KMeans(n_clusters=20, random_state=0).fit(nmf_ma)\n",
        "nmf_me_p=nmf_me.predict(nmf_ma)\n",
        "lsa_me= KMeans(n_clusters=20, random_state=0).fit(X2)\n",
        "lsa_me_p=lsa_me.predict(X2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn5Kngyj1xgh",
        "colab_type": "code",
        "outputId": "d581dc84-2503-495e-cec9-fa47f4a06377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "km_j(sampletag,cv_me_p,tf)\n",
        "calculate_result(sampletag,cv_me_p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.033\n",
            "Completeness: 0.294\n",
            "V-measure: 0.060\n",
            "Silhouette Coefficient: 0.580\n",
            "predict info:\n",
            "precision:0.070\n",
            "recall:0.036\n",
            "f1-score:0.006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLtOGz3NrOr9",
        "colab_type": "code",
        "outputId": "8c613a13-cceb-4ea3-ca76-60032fdcfe1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "km_j(sampletag,tf_me_p,tfidf)\n",
        "calculate_result(sampletag,tf_me_p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.219\n",
            "Completeness: 0.230\n",
            "V-measure: 0.224\n",
            "Silhouette Coefficient: -0.012\n",
            "predict info:\n",
            "precision:0.088\n",
            "recall:0.065\n",
            "f1-score:0.072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdLx-ZNd9bZ4",
        "colab_type": "code",
        "outputId": "7859ed57-a616-4dfd-f261-4f35306fd0f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "km_j(sampletag,nmf_me_p,nmf_ma)\n",
        "calculate_result(sampletag,nmf_me_p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.185\n",
            "Completeness: 0.236\n",
            "V-measure: 0.208\n",
            "Silhouette Coefficient: 0.312\n",
            "predict info:\n",
            "precision:0.020\n",
            "recall:0.038\n",
            "f1-score:0.021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeDd4ohV9HTL",
        "colab_type": "code",
        "outputId": "9ce8a793-0780-4ec9-f799-7a8dc75b5dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "km_j(sampletag,lsa_me_p,X2)\n",
        "calculate_result(sampletag,lsa_me_p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.264\n",
            "Completeness: 0.283\n",
            "V-measure: 0.273\n",
            "Silhouette Coefficient: 0.151\n",
            "predict info:\n",
            "precision:0.027\n",
            "recall:0.045\n",
            "f1-score:0.032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51qDkUH7N-9c",
        "colab_type": "text"
      },
      "source": [
        "6.Examine the clusters. For each cluster, count the categories associated with each document. Do you notice that a category tends to mainly dominate a cluster? Which categories tend to co-occur with one another?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDS2nYOdLBUP",
        "colab_type": "code",
        "outputId": "6a43af01-4b61-4d0e-8cf4-058c33033b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "cv_ac_c = Counter(cv_ac_p)\n",
        "cv_ac_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 3,\n",
              "         1: 41,\n",
              "         2: 2,\n",
              "         3: 937,\n",
              "         4: 1,\n",
              "         5: 1,\n",
              "         6: 1,\n",
              "         7: 1,\n",
              "         8: 2,\n",
              "         9: 1,\n",
              "         10: 1,\n",
              "         11: 1,\n",
              "         12: 1,\n",
              "         13: 1,\n",
              "         14: 1,\n",
              "         15: 1,\n",
              "         16: 1,\n",
              "         17: 1,\n",
              "         18: 1,\n",
              "         19: 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMqD8Lh1MzP5",
        "colab_type": "text"
      },
      "source": [
        "categories 3 is dominate in this cluster "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxoAmKC-LX6J",
        "colab_type": "code",
        "outputId": "a8fcceaf-1302-4c0c-cf88-bfdef3827520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "tf_ac_c = Counter(tf_ac_p)\n",
        "tf_ac_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 31,\n",
              "         1: 473,\n",
              "         2: 42,\n",
              "         3: 120,\n",
              "         4: 24,\n",
              "         5: 85,\n",
              "         6: 17,\n",
              "         7: 30,\n",
              "         8: 16,\n",
              "         9: 12,\n",
              "         10: 22,\n",
              "         11: 13,\n",
              "         12: 20,\n",
              "         13: 7,\n",
              "         14: 23,\n",
              "         15: 15,\n",
              "         16: 29,\n",
              "         17: 7,\n",
              "         18: 7,\n",
              "         19: 7})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Cv16ACNDFV",
        "colab_type": "text"
      },
      "source": [
        "categories 1 is dominate in this cluster, sencond is 3, it distribute better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5sBRwQPLeT1",
        "colab_type": "code",
        "outputId": "123f9152-2207-4aa4-97a2-243ef7e63b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "nmf_ac_c = Counter(nmf_ac_p)\n",
        "nmf_ac_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 27,\n",
              "         1: 49,\n",
              "         2: 34,\n",
              "         3: 50,\n",
              "         4: 310,\n",
              "         5: 81,\n",
              "         6: 20,\n",
              "         7: 19,\n",
              "         8: 90,\n",
              "         9: 50,\n",
              "         10: 4,\n",
              "         11: 8,\n",
              "         12: 11,\n",
              "         13: 4,\n",
              "         14: 8,\n",
              "         15: 17,\n",
              "         16: 26,\n",
              "         17: 2,\n",
              "         18: 18,\n",
              "         19: 172})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQtZjBXtNN-j",
        "colab_type": "text"
      },
      "source": [
        "categories 4 is dominate in this cluster, come after with categories 19 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8KZXLArLlOS",
        "colab_type": "code",
        "outputId": "f4936574-44ea-45eb-8bf8-5f7f6937d23f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "lsa_ac_c = Counter(lsa_ac_p)\n",
        "lsa_ac_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 84,\n",
              "         1: 46,\n",
              "         2: 126,\n",
              "         3: 137,\n",
              "         4: 27,\n",
              "         5: 47,\n",
              "         6: 32,\n",
              "         7: 65,\n",
              "         8: 32,\n",
              "         9: 26,\n",
              "         10: 15,\n",
              "         11: 92,\n",
              "         12: 36,\n",
              "         13: 58,\n",
              "         14: 22,\n",
              "         15: 32,\n",
              "         16: 27,\n",
              "         17: 48,\n",
              "         18: 38,\n",
              "         19: 10})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldn-V15kNc4Q",
        "colab_type": "text"
      },
      "source": [
        "categories 3 is dominate in this cluster then is categories 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_CNHKLtLsCm",
        "colab_type": "code",
        "outputId": "2d75f0ea-286e-4449-952a-b56ae71f575a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "cv_me_c = Counter(cv_me_p)\n",
        "cv_me_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 47,\n",
              "         1: 1,\n",
              "         2: 1,\n",
              "         3: 1,\n",
              "         4: 1,\n",
              "         5: 1,\n",
              "         6: 1,\n",
              "         7: 1,\n",
              "         8: 1,\n",
              "         9: 1,\n",
              "         10: 1,\n",
              "         11: 2,\n",
              "         12: 1,\n",
              "         13: 1,\n",
              "         14: 1,\n",
              "         15: 1,\n",
              "         16: 934,\n",
              "         17: 1,\n",
              "         18: 1,\n",
              "         19: 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2m9a2OENlBk",
        "colab_type": "text"
      },
      "source": [
        "categories 16 is dominate in this cluster "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w22nlCrYLxUp",
        "colab_type": "code",
        "outputId": "afe40e3b-dfaa-44a8-c358-2f53b8d7d615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "tf_me_c = Counter(tf_me_p)\n",
        "tf_me_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 28,\n",
              "         1: 79,\n",
              "         2: 27,\n",
              "         3: 56,\n",
              "         4: 36,\n",
              "         5: 38,\n",
              "         6: 167,\n",
              "         7: 58,\n",
              "         8: 50,\n",
              "         9: 28,\n",
              "         10: 58,\n",
              "         11: 75,\n",
              "         12: 34,\n",
              "         13: 24,\n",
              "         14: 62,\n",
              "         15: 43,\n",
              "         16: 17,\n",
              "         17: 39,\n",
              "         18: 59,\n",
              "         19: 22})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAwphcSONzaO",
        "colab_type": "text"
      },
      "source": [
        "categories 6 is dominate in this cluster "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF2SSWDlL2o2",
        "colab_type": "code",
        "outputId": "7ae3d566-0c29-4cd3-9f0a-57964c511517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "nmf_me_c = Counter(nmf_me_p)\n",
        "nmf_me_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 261,\n",
              "         1: 8,\n",
              "         2: 21,\n",
              "         3: 15,\n",
              "         4: 16,\n",
              "         5: 21,\n",
              "         6: 6,\n",
              "         7: 41,\n",
              "         8: 61,\n",
              "         9: 233,\n",
              "         10: 30,\n",
              "         11: 12,\n",
              "         12: 21,\n",
              "         13: 18,\n",
              "         14: 4,\n",
              "         15: 8,\n",
              "         16: 22,\n",
              "         17: 109,\n",
              "         18: 40,\n",
              "         19: 53})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHEx5ja0N6Rz",
        "colab_type": "text"
      },
      "source": [
        "categories 0 is dominate in this cluster come with 9 then is 17"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lqrZvWXL-fs",
        "colab_type": "code",
        "outputId": "dcf50e7a-e730-438e-f1a2-6528238955cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "lsa_me_c = Counter(lsa_me_p)\n",
        "lsa_me_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 45,\n",
              "         1: 40,\n",
              "         2: 17,\n",
              "         3: 170,\n",
              "         4: 10,\n",
              "         5: 30,\n",
              "         6: 39,\n",
              "         7: 55,\n",
              "         8: 31,\n",
              "         9: 51,\n",
              "         10: 12,\n",
              "         11: 90,\n",
              "         12: 48,\n",
              "         13: 28,\n",
              "         14: 109,\n",
              "         15: 18,\n",
              "         16: 44,\n",
              "         17: 51,\n",
              "         18: 62,\n",
              "         19: 50})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JvFtCYcOIHf",
        "colab_type": "text"
      },
      "source": [
        "categories 3 is dominate in this cluster then come with 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LN5kPAgtB2-",
        "colab_type": "text"
      },
      "source": [
        "<H3> Memo </H3>\n",
        "\n",
        "7. Write up your methodology/findings in a memo. (Recall the guidelines for memos from a previous HW.) Which combination of algorithms and parameter inputs resulted in the best grouping of documents? How well does each clustering algorithm isolate the individual categories? [Think up more insights to write about!] *The memo should be a separate document from this notebook.*"
      ]
    }
  ]
}