{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mid_exam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "10MoNPUBSDSNrx31T8ctb-lWjKgXivrAR",
      "authorship_tag": "ABX9TyM0P3HbGn9Q4RCaEWmszTn9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEzgGGEYOfty",
        "colab_type": "code",
        "outputId": "6568f0b1-8f3c-4c07-8e7f-ab24d2df7599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words= set(stopwords.words('english'))\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRPlnfna1y46",
        "colab_type": "text"
      },
      "source": [
        "Task 1: (15 pts.)  \n",
        "Without using anything but the raw json data (i.e., via the variable \"data\", and not the pandas dataframe \"A\"), write code to find the counts of all unique question categories. Your answer should match the above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIJz3raEulzj",
        "colab_type": "code",
        "outputId": "5b9afacc-1c6a-48d2-d0fa-62153c6f386d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#Q1\n",
        "import json\n",
        "with open('/content/drive/My Drive/NLP/exam/JEOPARDY_QUESTIONS1.json', 'r') as f:      \n",
        "    rawj = json.load(f) \n",
        "data = pd.DataFrame(rawj)  \n",
        "\n",
        "from collections import Counter  \n",
        "ct=Counter([x['category'] for x in rawj if x.get('category')])#\\'category\\':.*?(?=,)\n",
        "ct=ct.most_common(len(ct))\n",
        "ct[0:20]#for easy read only output 20"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('BEFORE & AFTER', 547),\n",
              " ('SCIENCE', 519),\n",
              " ('LITERATURE', 496),\n",
              " ('AMERICAN HISTORY', 418),\n",
              " ('POTPOURRI', 401),\n",
              " ('WORLD HISTORY', 377),\n",
              " ('WORD ORIGINS', 371),\n",
              " ('COLLEGES & UNIVERSITIES', 351),\n",
              " ('HISTORY', 349),\n",
              " ('SPORTS', 342),\n",
              " ('U.S. CITIES', 339),\n",
              " ('WORLD GEOGRAPHY', 338),\n",
              " ('BODIES OF WATER', 327),\n",
              " ('ANIMALS', 324),\n",
              " ('STATE CAPITALS', 314),\n",
              " ('BUSINESS & INDUSTRY', 311),\n",
              " ('ISLANDS', 301),\n",
              " ('WORLD CAPITALS', 300),\n",
              " ('U.S. GEOGRAPHY', 299),\n",
              " ('RELIGION', 297)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__PgsyRF2RE3",
        "colab_type": "text"
      },
      "source": [
        "Task 2: (30 pts.)  \n",
        "Using the pandas dataframe \"A\", write code to see if there is a correlation between question length (in character counts) and the dollar value of the question. Hint: use the Pearson correlation via scipy.stats:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_1RW6mwokBj",
        "colab_type": "code",
        "outputId": "686e99cd-6a20-4f4a-cd43-9990dcf4c244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Q2\n",
        "data1=data\n",
        "data1['value1'] = data['value'].str.extract('\\$(\\d+)', expand=False).astype(float)#clean digital\n",
        "data1[\"question_clean\"]=data1[\"question\"].str.replace(r'[^\\w\\s]+', '')#remove Punctuation\n",
        "data1[\"length\"]= data1[\"question_clean\"].str.len() \n",
        "data1=data1.fillna(0)\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "x = np.asarray(data1['value1'])\n",
        "y = np.asarray(data1['length'])\n",
        "print(pearsonr(x,y))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.10364886009811222, 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BazDtJCu2Xy6",
        "colab_type": "text"
      },
      "source": [
        "r=0.1112,p-value=0, p value is very small, we may reject H0 there a liitle connection between character len and value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynrFzCPv6gmZ",
        "colab_type": "text"
      },
      "source": [
        "Task 3: (20 pts.)\n",
        "Write code to count the number of adjectives that begin with a vowel contained in all questions asked in 2006."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiCIhRdgWhF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#q3\n",
        "def getlen(x):\n",
        "  x=x.lower()\n",
        "  tags = set(['JJ', 'JJR', 'JJS'])\n",
        "  x=word_tokenize(x)\n",
        "  #print(x)\n",
        "  pos_tags =nltk.pos_tag(x)\n",
        "  #print(pos_tags)\n",
        "  ret = []\n",
        "  for word,pos in pos_tags:\n",
        "    if (pos in tags and re.match(\"^[a|e|i|o|u]\",word)):\n",
        "      ret.append(word)\n",
        "  #print(ret)\n",
        "  return(len(ret))\n",
        "data2006=data.loc[(data['air_date'].str.startswith('2006'))].reset_index(drop=True)\n",
        "data2006[\"adj_vow\"]=data2006[\"question\"].apply(lambda x :getlen(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV_ZhyqeqSUC",
        "colab_type": "code",
        "outputId": "b3e56323-dc88-4555-8aa2-39359072c225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total=sum(data2006[\"adj_vow\"])\n",
        "total"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3860"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PArKBHRn6-kN",
        "colab_type": "text"
      },
      "source": [
        "Task 4: (35 pts.)  \n",
        "Train two binary classification models to predict whether or not a question is either a LITERATURE or SCIENCE question, using the k-nearest neighbor classifier and the ridge classifier.\n",
        "\n",
        "Report out-of-sample accuracy, recall, precision, and f1 scores of both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZplNULCMZkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Q4\n",
        "knntest=data[['question','category']]\n",
        "\n",
        "#may not use\n",
        "def clean_text(croup):\n",
        "  x=word_tokenize(croup)\n",
        "  clean_senteces=pd.Series(x).str.replace(\"[^a-zA-Z0-9]\",\" \")\n",
        "  clean_senteces=[s.lower() for s in clean_senteces]\n",
        "  text_no_sp=[]\n",
        "  for r in clean_senteces:\n",
        "    tmp=r.split()\n",
        "    sen_new=\" \".join([i for i in tmp if i not in stop_words ])\n",
        "    #sen_new=\" \".join([i for i in tmp])\n",
        "    if len(sen_new)>0:\n",
        "      text_no_sp.append(sen_new)\n",
        "  text_no_sp=\" \".join(text_no_sp)\n",
        "  return(text_no_sp)\n",
        "  #return (clean_senteces)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4efsamh-tkE",
        "colab_type": "code",
        "outputId": "140640dc-84ca-4ec0-8348-4a474728b6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#knntest[\"question1\"]=knntest[\"question\"].apply(lambda x :clean_text(x))\n",
        "#knntest=knntest.loc[knntest['category']==\"SCIENCE\"].reset_index(drop=True)\n",
        "knn=knntest.loc[knntest['category'].isin(['SCIENCE','LITERATURE'])  ].reset_index(drop=True)\n",
        "knntest_label=np.asarray(knn[\"category\"])\n",
        "knn[\"question_cl\"]=knn[\"question\"].apply(lambda x :clean_text(x))\n",
        "len(knn)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfeSfs4sA5ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "v = TfidfVectorizer(stop_words=set(stop_words))\n",
        "knntest_set = v.fit_transform(knn[\"question_cl\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCHqksDTF7sD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import neighbors     \n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=3, algorithm='auto', weights='distance', n_jobs=1)\n",
        "knn.fit(knntest_set[0:900],knntest_label[0:900])\n",
        "pred_label=knn.predict(knntest_set[900:1000])\n",
        "insamp=knn.predict(knntest_set[0:900])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSAtzmBqY-68",
        "colab_type": "code",
        "outputId": "7cb9f756-3ecf-4ac0-c47f-7754afda72f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "def metrics_result(actual, predict):\n",
        "  print('precision:{0:.3f}'.format(metrics.precision_score(actual, predict, average='weighted')))\n",
        "  print('recall:{0:0.3f}'.format(metrics.recall_score(actual, predict, average='weighted')))\n",
        "  print('f1-score:{0:.3f}'.format(metrics.f1_score(actual, predict, average='weighted')))\n",
        "print(\"train:0~900,test:900~1000\")\n",
        "print(\"=======out of sample========\")\n",
        "metrics_result(knntest_label[900:1000], pred_label)\n",
        "print(\"==========in sample==========\")\n",
        "metrics_result(knntest_label[0:900], insamp)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:0~900,test:900~1000\n",
            "=======out of sample========\n",
            "precision:0.890\n",
            "recall:0.890\n",
            "f1-score:0.890\n",
            "==========in sample==========\n",
            "precision:1.000\n",
            "recall:1.000\n",
            "f1-score:1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOkIa0ylrzhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rilabel=[]\n",
        "for i in knntest_label:\n",
        "  if i==\"SCIENCE\":\n",
        "    rilabel.append(0)#sci to 0\n",
        "  else:\n",
        "    rilabel.append(1)\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "clf =RidgeClassifier()\n",
        "clf.fit(knntest_set[0:900],rilabel[0:900]) \n",
        "clfpre=clf.predict(knntest_set[900:1000])\n",
        "incl=clf.predict(knntest_set[0:900])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGeu_Cb00tYV",
        "colab_type": "code",
        "outputId": "16d27fa3-324e-4398-ce32-27e29e709b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(\"train:0~900,test:900~1000\")\n",
        "print(\"=======out of sample========\")\n",
        "metrics_result(rilabel[900:1000], clfpre)\n",
        "print(\"==========in sample==========\")\n",
        "metrics_result(rilabel[0:900], incl)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:0~900,test:900~1000\n",
            "=======out of sample========\n",
            "precision:0.961\n",
            "recall:0.960\n",
            "f1-score:0.960\n",
            "==========in sample==========\n",
            "precision:1.000\n",
            "recall:1.000\n",
            "f1-score:1.000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}